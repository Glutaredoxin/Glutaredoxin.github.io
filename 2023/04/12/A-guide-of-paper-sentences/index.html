<!DOCTYPE html>
<html>
    <!-- Head -->
    <head>
    <meta http-equiv="content-type" content="text/html; charset=utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <meta name="author" content="Glutaredoxin">
    <meta name="description" itemprop="description" content="">
    <meta name="keywords" content="">

    <!-- Page Title -->
    
        <title>A guide of paper sentences | Glutaredoxin&#39;log</title>
    
    <link rel="icon" href="/img/strawberry.png">
    <link rel="stylesheet" href="https://cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.css">
    <script src="https://cdn.staticfile.org/jquery/3.2.1/jquery.min.js"></script>
    <link href="https://cdn.bootcss.com/bootstrap/3.3.7/css/bootstrap.min.css" rel="stylesheet">
    
    <script src="//cdn.bootcss.com/prettify/r298/prettify.min.js" type="text/javascript"></script>
    <link rel="stylesheet" href="/css/google-prettify-monokai.css" type="text/css">
    
    
<script src="/js/script.js"></script>

    
<link rel="stylesheet" href="/css/style.css">

    
    <style>
        .deactiveColor{
            color: #37474f;
        }
        .activeColor{
            color: #006064;
        }
        a:hover{
            color: #006064;
        }
        .header-btn{
            color: #37474f;
        }
        
        .post-content img{
            margin: 50px auto;
        }
        
    </style>
<meta name="generator" content="Hexo 7.3.0"></head>
    <body>
        <div class="container">

            <!-- Top Anchor -->
            <div id="top"></div>

            <!-- Header -->
            <header class="header-wrapper">
    <div class="header-title-wrapper">
        <!-- Page Title -->
        <p class="header-title">
             
                
                    A guide of paper sentences
                
            
        </p>  
    </div>    

    
        <!-- Division Line -->
        <div class="division"></div> 
    
    
    <div class="header-detail">
        <!-- Header Button -->
        <div class="header-btn-wrapper">
            
                <span>
                    <a class="home-btn header-btn" href="/" title="homepage"><i class="fa fa-home"></i></a>
                </span>

                
                    <span>
                        <a class="catalog-btn header-btn"><i class="fa fa-list-ul"></i></a>
                    </span>
                
            
        </div>
    </div>
</header>

            <!-- Main -->
            <main>
                <article class="post-wrapper">
    

    
        <!-- Article Catalog -->
        <div class="catalog-dropdown col-xs-12 col-sm-12">
            <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%F0%9F%96%8A%EF%B8%8FA-guide-of-paper-sentences"><span class="toc-number">1.</span> <span class="toc-text">🖊️A guide of paper sentences</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%F0%9F%91%89limited-data"><span class="toc-number">1.1.</span> <span class="toc-text">👉limited data</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%F0%9F%94%B8Self-supervised-learning"><span class="toc-number">1.1.1.</span> <span class="toc-text">🔸Self-supervised learning:</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%F0%9F%91%89-Contrastive-Representation-Learning"><span class="toc-number">1.2.</span> <span class="toc-text">👉 Contrastive Representation Learning</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#lefted-structured-loss-2015-%E7%BB%93%E6%9E%84%E5%8C%96%E6%8D%9F%E5%A4%B1"><span class="toc-number">1.2.1.</span> <span class="toc-text">lefted structured loss (2015)[结构化损失]:</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Multi-class-N-pair-loss-2016"><span class="toc-number">1.2.2.</span> <span class="toc-text">Multi-class N-pair loss (2016):</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#NCE-Noise-Contrastive-Estimation"><span class="toc-number">1.2.3.</span> <span class="toc-text">NCE (Noise Contrastive Estimation):</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%F0%9F%8E%89some-localism"><span class="toc-number">1.3.</span> <span class="toc-text">🎉some localism</span></a></li></ol></li></ol>
        </div>
    

    
        <!--For now, Lightbox Only Show in Post Layout -->
        

        
        
            <!-- Article Img Lightbox -->
            <div class="gallery">
    
    
    <div class="lightbox">
        <!-- Close Button -->
        <span class="close-gallery">&times;</span>
        
        <!-- Photo -->
        <img class="gallery-photo">
    </div>
</div>
        
    

    

    <!-- Article Content -->
    <div class="post-content">
        <h1 id="🖊️A-guide-of-paper-sentences"><a href="#🖊️A-guide-of-paper-sentences" class="headerlink" title="🖊️A guide of paper sentences"></a>🖊️A guide of paper sentences</h1><h2 id="👉limited-data"><a href="#👉limited-data" class="headerlink" title="👉limited data"></a>👉limited data</h2><ol>
<li>
Given a task and enough labels, superivsed learning can solve it really well. Good performance usually requires a decent amount of labels, but collecting **mutual labels** is expensive.
</li>
<li>
Producing a dataset with clean labels is expensive but unlabeled data is being generated all the time.
</li>
</ol>

<h3 id="🔸Self-supervised-learning"><a href="#🔸Self-supervised-learning" class="headerlink" title="🔸Self-supervised learning:"></a>🔸Self-supervised learning:</h3><p>We are interested in the learned intermediate representation with expectation that this representation can carry good semantic or structural meanings and can be beneficial to a variety of practical downstream tasks.</p>
<p>Like we train the auxiliary model to predict which images are rotated. Acutually, the accuracy is unimportant. Rather we expect the model to learn <strong>high-quality latent variables</strong> for real-world tasks. </p>
<p>All the generative models can be considered as self-supervised with different goals. [Assume: tasks according to the self images]</p>
<p>Recently, some researchers proposed to train supervised learning on labelled data and self-supervised pretext tasks on unlabelled data simultaneously with shared weights.</p>
<table>
<thead>
<tr>
<th>method</th>
<th>description</th>
</tr>
</thead>
<tbody><tr>
<td>Distortion</td>
<td>small distortion on a image does not modify its original semantic meaning or geometric forms.</td>
</tr>
<tr>
<td>patches</td>
<td>extract multiple patches from one image and ask the model to predict the relationship between these patches.[DeepEmd]</td>
</tr>
<tr>
<td>colorization</td>
<td></td>
</tr>
<tr>
<td>generating model</td>
<td>reconstruct thr original input while learning meaningful latent representation</td>
</tr>
<tr>
<td>contrastive learning</td>
<td>↩🔹</td>
</tr>
</tbody></table>
<p>↩🔹contrastive learning – <strong>CPC (Contrastive Predictive Coding)</strong></p>
<p>We expect the feature representations are high-dimensionally, which <strong>focuses more on global and semantic information</strong> rather in the pixel levels. </p>
<p>CPC utilizes the relavance between samples in sequence to excavate the feature of features. <strong>The single image can obtain the inner relavance by cutting in a certain direction.</strong></p>
<p>Center on single image:<br><img src="https://raw.githubusercontent.com/Glutaredoxin/image/main/CPC.jpg" alt="CPC"></p>
<p>Sampling the overlapped patches from the single image as relavance sequence $[x_1, x_2, …, x_n]$.</p>
<p>Positive sample: <strong>sequential sampling</strong></p>
<p>$[x_{t-3}, x_{t-2}, x_{t-1}, x_{t}]$ -&gt; $[x_{t+1}, x_{t+2}, x_{t+3}, x_{t+4}]$</p>
<p>utilize the first 4 to predict the last 4.</p>
<p>Negative sample: <strong>random sampling</strong><br>$[x_{t-3}, x_{t-2}, x_{t-1}, x_{t}]$ -&gt; $[x_{a}, x_{b}, x_{c}, x_{d}]$<br><img src="https://raw.githubusercontent.com/Glutaredoxin/image/main/CPC%20framework.jpg" alt="CPC_framework"></p>
<p>Input anchor to encoder to obtain $[z_{t-3}, z_{t-2}, z_{t-1}, z_{t}]$, and then feed into 循环网络 to contxt vector $c_t$. <strong>It noted that $c_t$ is the feature across the multiple time steps.</strong></p>
<p>$z_t$ and $c_t$ can be the feature representations of $x_t$. $z_t$ focuses on the single step and $c_t$ focuses on the multiple steps.</p>
<h2 id="👉-Contrastive-Representation-Learning"><a href="#👉-Contrastive-Representation-Learning" class="headerlink" title="👉 Contrastive Representation Learning"></a>👉 Contrastive Representation Learning</h2><ol>
<li>
The goal of contrastive representation learning is to learn such an **embedding space** in which **similar sample pairs stay close to each other while dissimilar ones are far away**.
</li>
<li>
Contrastive learning can be applied to both supervised and unsupervised settings. And **contrastive learning with unsupervised data is the most powerful approaches in self-supervised learning**.
</li>
</ol>

<h3 id="lefted-structured-loss-2015-结构化损失"><a href="#lefted-structured-loss-2015-结构化损失" class="headerlink" title="lefted structured loss (2015)[结构化损失]:"></a>lefted structured loss (2015)[结构化损失]:</h3><p>unlike contrastive loss or triplet loss choose one negative, lefted structured loss chooses the all negative in mini-batch to compute loss.</p>
<p>$$L &#x3D; \frac{1}{2|P|}\sum_{(i,j)\in P}max(0, L_{i,j})^2$$</p>
<p>$$L_{i,j}&#x3D;max(max_{(i,k)\in N}(\alpha-D_{i,k}), max_{(j,k) \in N}(\alpha-D_{j,k})) + D_{i,j}$$</p>
<p>So, loss is composed by the minimal distance from i or from j, and the embedding distance between i and j.<br><strong>The lifted structure loss will considers all negative samples in mini-batch for each positive samples i and j.</strong></p>
<h3 id="Multi-class-N-pair-loss-2016"><a href="#Multi-class-N-pair-loss-2016" class="headerlink" title="Multi-class N-pair loss (2016):"></a>Multi-class N-pair loss (2016):</h3><p>$$L_{n-pair}(x, x^{+}, {x_i^{-}}<em>{i&#x3D;1}^{N-1})&#x3D;-log \frac{exp(f(x)^Tf(x^+))}{exp(f(x)^Tf(x^+)) + \sum</em>{i&#x3D;1}^{N-1}exp(f(x)^Tf(x_i^{-}))}$$</p>
<p>we can see, if we only sample one negative sample per class, it is euivalent to the <strong>softmax loss for multi-class classification</strong>.</p>
<h3 id="NCE-Noise-Contrastive-Estimation"><a href="#NCE-Noise-Contrastive-Estimation" class="headerlink" title="NCE (Noise Contrastive Estimation):"></a>NCE (Noise Contrastive Estimation):</h3><p>run logistic regression to tell apart the target data from noise.</p>
<ol>
<li>
InfoNCE(2018) -- CPC
</li>
<li>
Soft-Nearest Neighbors loss(2019)
</li>
</ol>
The relationship of InfoNCE and cross-entropy loss:

<p>Let’s start with softmax:<br>$$\hat{y_{+}}&#x3D;softmax(z_{+})&#x3D;\frac{exp(z_{+})}{\sum_{i&#x3D;0}^{k}exp(z_{i})}$$</p>
<p>and cross-entropy loss:<br>$$L(\hat{y})&#x3D;-\sum_{i \in K}y_ilog(\hat{y_i})&#x3D;-log\frac{exp(z_{+})}{\sum_{i&#x3D;0}^{k}exp(z_{i})}$$</p>
<p>and k is the number of classes.</p>
<p>if we use cross-entropy to compute the contrastive learning, it is unsuccessful. Because through data augmentation, <strong>the number of images we have, the number of classes we have,</strong> and the dimentional of one-hot vector is so high.</p>
<p>So we introduce the NCE loss. we <strong>split the multi-classification into binary-classification</strong>. one class is data sample and another is noisy sample. 我们将数据样本与噪声样本进行比较，根据噪声对比，发现数据中的某些特性。当然，如果将整个数据集剩下的数据都当作负样本，计算复杂度很高，所以我们对负样本进行采样。当然，负样本选取越多，越接近整个数据集，效果越好。</p>
<p>但是，NCE Loss只是二分类，也就是将所有负样本都当作一个类，但其实负样本是由多个类组成的，所以还是当成多分类任务比较好。</p>
<p>$$L_q &#x3D; -log\frac{exp(q k_{+})}{\sum_{i&#x3D;0}^{k}exp(q k_{i})}$$</p>
<p>这里的k其实就是采样的负样本数量。</p>
<h2 id="🎉some-localism"><a href="#🎉some-localism" class="headerlink" title="🎉some localism"></a>🎉some localism</h2><ol>
<li>
The motivation is quite straightforward.
</li>
<li>
Broadly speaking, 
</li>
<li>
NXX CXX, short for NC. (简称)
</li>
<li>
tell apart A from B. 区分A和B
</li>
<li>
XXX is an important stepping stone towards XXX. (Unsupervised learning is an important stepping stone towards robust and generic representation learning.)垫脚石。
</li>
<li>
remain elusive. 是难以理解的。
</li>
<li>
A good representation should be simple and compact with good generalization ability to avoid overfitting.
</li>
<li>
XXX needs only one clean and efficient step.
</li>
<li>
底层的特征对于小物体检测是非常有帮助的。 -- FPN
</li>
</ol>  
    </div> 

    
        <!-- Division Line -->
        <div class="division"></div> 
    

    <div class="post-info-wrapper">
            
                    <!-- Post Info -->
                    <p class="post-date">2023-04-12</p>
                    
                    
                        <p class="post-info-categories">
                            
                        </p>
                    

                    
                        <p class="post-info-tags">
                            
                        </p>
                    
            
    </div>
</article>


    

            </main>

            <!-- 'To Top' Btn-->
            
                <div id="to-top">
    <a href="#top" class="toTop">
        <i class="fa fa-pagelines"></i>
    </a>
</div>
            

            <!-- Footer -->
            
                <footer class="footer-wrapper col-xs-12 col-sm-12">
    <div class="footer-banner-wrapper">
        <p class="footer-banner">Powered by <a target="_blank" rel="noopener" href="https://hexo.io/" title="Hexo">Hexo</a></p>
        <P class="footer-banner">Theme <a target="_blank" rel="noopener" href="https://github.com/Lonezj/hexo-theme-wind" title="Wind">wind</a></P>
    </div>
</footer>
            
        </div>
    </body>
</html>